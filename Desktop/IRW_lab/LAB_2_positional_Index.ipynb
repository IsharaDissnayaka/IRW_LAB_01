{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2A1l9dA4MBS",
        "outputId": "819bc6cc-2b88-4b3b-f97d-91ac8f86f5fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/IRWA/Lab_2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTzFi2Ci6PXj",
        "outputId": "54acfd3a-f6e3-4492-a819-efe1a13773e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/IRWA/Lab_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download stopsword\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#download tokanizer\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSLhM8rg2iJ3",
        "outputId": "e31e9215-cc81-4258-a03b-98f0452b6614"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import DictionaryConditionalProbDist\n",
        "\n",
        "import os\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "dictionary = dict()\n",
        "def positional():\n",
        "  for doc1 in range(1,4):\n",
        "    with open(str(os.getcwd())+'/positional/doc_'+str(doc1)+'.txt','r') as file:\n",
        "      s=file.read().replace('\\n','')[1:]\n",
        "      #print('sentence:-',s)\n",
        "      s = re.sub(r\"won\\'t\",\"will not\",s)\n",
        "      s = re.sub(r\"can\\'t\",'can not',s)\n",
        "      s = re.sub(r\"n\\'t\",\"not\",s)\n",
        "      s = re.sub(r\"\\'re\",\"are\",s)\n",
        "      s = re.sub(r\"\\'s\",\"is\",s)\n",
        "      s = re.sub(r\"\\'d\",\"would\",s)\n",
        "      s = re.sub(r\"\\'ll\",\"will\",s)\n",
        "      s = re.sub(r\"\\'t\",\"not\",s)\n",
        "      s = re.sub(r\"\\'ve\",\"have\",s)\n",
        "      s = re.sub(r\"\\'t\",\"not\",s)\n",
        "      s = re.sub(r\"\\'am\",\"am\",s)\n",
        "      s = re.sub(r'[^\\w\\s]','',s)\n",
        "\n",
        "      #whole word seperate one by one words\n",
        "      words = word_tokenize(str(s).lower())\n",
        "      #print(words)\n",
        "      stop_words=stopwords.words('english')\n",
        "      new_sentence=[a for a in words if a not in stop_words ]\n",
        "      #print(new_sentence)\n",
        "      #for word_tokenzed in words:\n",
        "        # if stop_words not in word_tokenzed:\n",
        "        #    new_sentence.append(word_tokenzed)\n",
        "\n",
        "      stemmed = []\n",
        "      ps = PorterStemmer()\n",
        "      for i in new_sentence:\n",
        "        stemmed.append(ps.stem(i))\n",
        "      #print(stemmed)\n",
        "\n",
        "\n",
        "      #postional indexing\n",
        "      #create teparely dictory and store stemmed value\n",
        "\n",
        "      temp_dict = dict()\n",
        "      a=0\n",
        "\n",
        "      for key in stemmed:\n",
        "        temp_dict.setdefault(key,[])\n",
        "        temp_dict[key].append(a)\n",
        "        a +=1\n",
        "\n",
        "        for x in temp_dict:\n",
        "          if dictionary.get(x):\n",
        "            dictionary[x][doc1]=temp_dict.get(x)\n",
        "          else:\n",
        "            dictionary.setdefault(key,[])\n",
        "            dictionary[key]={}\n",
        "            dictionary[x][doc1]=temp_dict.get(x)\n",
        "  return positional\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mh7Pgcn-6XOw"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(positional())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJbGZ4t0JbiI",
        "outputId": "27037636-831a-4983-bba2-1fcaeb1cb33b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function positional at 0x7f13887e8160>\n"
          ]
        }
      ]
    }
  ]
}